# Scrape n Bake
<!-- badges -->
![PyPI - Status](https://img.shields.io/pypi/status/wheel)
![GitHub repo size](https://img.shields.io/github/repo-size/leetheperm/Scrape_n_bake)
![no python2](https://img.shields.io/badge/python%202-not%20available-red)
![MIT license](https://img.shields.io/badge/license-MIT-lightgrey)
![Selenium]https://img.shields.io/pypi/v/selenium

## Objective

The objective of this project is to create a web scraping application that can be used for the most common use cases 


### prerequisites

 ![python version](https://img.shields.io/badge/python-3.7.4-green)

![BeautifulSoup](https://img.shields.io/pypi/v/bs4)  Beautiful Soup


 ![Selenium](https://img.shields.io/pypi/v/selenium)  Selenium Webdriver

 Geckodriver


### Future updates

<!-- feel free to come up with ideas -->

- [ ] Cloud deployment (heroku)
- [ ] Django web scraping app
- [ ] Unit tests
- [ ] Python package
- [ ] write to csv files
- [ ] CI/CD


### arguments

<!-- based on terminal app, but will obviosuly change in time -->

-u url (pass one url into the command line)
-l list of urls (path to text file of urls)
-f findAll (pass arguments like h1 or p or similar)
-s selector (to be done later)
-r regex (to be done later)
-p preset + preset name (phonenum, email, )
-w write to file (give file path and file name)
-t telephone number

### examples

```
pyhton3 webscrape.py -u site.com -f "h1"
```
or
```
python3 webscrape.py -l /usr/share/listofsites -f "p"
```

options

--help has help menu

### Contributors

<!-- If you do commit a change, feel free to add your details in here -->

* Lee Davies
  * [Github](https://www.github.com/leetheperm)
  * [Facebook](https://www.facebook.com/groups/cypress.testers)
